{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad91baa6-0b35-4b32-a0ba-8cd59a6e07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08a12cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory growth enabled. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Memory Management: This prevents TensorFlow from hogging all VRAM at once\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print(\"GPU Memory growth enabled. Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de3dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully built for 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# --- Define labels and build model architecture ---\n",
    "\n",
    "# We must redefine labels in this notebook as kernels are independent\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', \n",
    "          'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', \n",
    "          'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "\n",
    "# 1. Load Pre-trained DenseNet121\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# 2. Add custom head for our specific 14 pathologies\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) \n",
    "x = Dropout(0.2)(x)             \n",
    "predictions = Dense(len(labels), activation='sigmoid')(x) \n",
    "\n",
    "# 3. Final model assembly\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# 4. Compile with optimized settings for 4GB VRAM\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.AUC(name='auc', multi_label=True)])\n",
    "\n",
    "print(f\"Model successfully built for {len(labels)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950aa791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks defined and ready.\n"
     ]
    }
   ],
   "source": [
    "import os # Import OS to handle folder creation\n",
    "\n",
    "# 1. Create a folder for saving weights safely\n",
    "if not os.path.exists('models'): \n",
    "    os.makedirs('models')\n",
    "    print(\"Created 'models' directory.\")\n",
    "\n",
    "# 2. Define the callback for saving the best model during training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='models/best_model.keras', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Define EarlyStopping to avoid over-training and wasting GPU time\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 4. Define Learning Rate reduction for finer tuning\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2, \n",
    "    patience=2, \n",
    "    verbose=1, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, reduce_lr]\n",
    "print(\"Callbacks defined and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108486c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89859 validated image filenames.\n",
      "Found 10987 validated image filenames.\n",
      "Generators Re-initialized in Notebook 03!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv('../data/cleaned_data.csv')\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', \n",
    "          'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', \n",
    "          'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "\n",
    "# 2. Split Data (Patient-Aware)\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, temp_idx = next(gss.split(df, groups=df['Patient ID']))\n",
    "train_df = df.iloc[train_idx]\n",
    "temp_df = df.iloc[temp_idx]\n",
    "\n",
    "gss_val = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(gss_val.split(temp_df, groups=temp_df['Patient ID']))\n",
    "valid_df = temp_df.iloc[val_idx]\n",
    "\n",
    "# 3. Create Generators (Optimized for 4GB VRAM)\n",
    "train_idg = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=15, zoom_range=0.1)\n",
    "test_idg = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_idg.flow_from_dataframe(dataframe=train_df, x_col='path', y_col=labels,\n",
    "                                          class_mode='raw', target_size=(224, 224), batch_size=4)\n",
    "\n",
    "valid_gen = test_idg.flow_from_dataframe(dataframe=valid_df, x_col='path', y_col=labels,\n",
    "                                         class_mode='raw', target_size=(224, 224), batch_size=4)\n",
    "\n",
    "print(\"Generators Re-initialized in Notebook 03!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e32aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... Go grab a coffee, this will take time!\n",
      "Epoch 1/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1696 - auc: 0.6962\n",
      "Epoch 1: val_loss improved from inf to 0.18059, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4335s 193ms/step - loss: 0.1696 - auc: 0.6962 - val_loss: 0.1806 - val_auc: 0.7421 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1598 - auc: 0.7482\n",
      "Epoch 2: val_loss improved from 0.18059 to 0.16052, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4180s 186ms/step - loss: 0.1598 - auc: 0.7482 - val_loss: 0.1605 - val_auc: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1561 - auc: 0.7705\n",
      "Epoch 3: val_loss did not improve from 0.16052\n",
      "22464/22464 [==============================] - 4181s 186ms/step - loss: 0.1561 - auc: 0.7705 - val_loss: 0.1837 - val_auc: 0.7779 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1538 - auc: 0.7820\n",
      "Epoch 4: val_loss did not improve from 0.16052\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "22464/22464 [==============================] - 4160s 185ms/step - loss: 0.1538 - auc: 0.7820 - val_loss: 0.1643 - val_auc: 0.8045 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1479 - auc: 0.8114\n",
      "Epoch 5: val_loss improved from 0.16052 to 0.15659, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4158s 185ms/step - loss: 0.1479 - auc: 0.8114 - val_loss: 0.1566 - val_auc: 0.8198 - lr: 2.0000e-05\n",
      "Epoch 6/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1462 - auc: 0.8195\n",
      "Epoch 6: val_loss did not improve from 0.15659\n",
      "22464/22464 [==============================] - 4218s 188ms/step - loss: 0.1462 - auc: 0.8195 - val_loss: 0.1568 - val_auc: 0.8260 - lr: 2.0000e-05\n",
      "Epoch 7/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1451 - auc: 0.8220\n",
      "Epoch 7: val_loss improved from 0.15659 to 0.15263, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4034s 180ms/step - loss: 0.1451 - auc: 0.8220 - val_loss: 0.1526 - val_auc: 0.8288 - lr: 2.0000e-05\n",
      "Epoch 8/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1446 - auc: 0.8259\n",
      "Epoch 8: val_loss did not improve from 0.15263\n",
      "22464/22464 [==============================] - 3993s 178ms/step - loss: 0.1446 - auc: 0.8259 - val_loss: 0.1561 - val_auc: 0.8135 - lr: 2.0000e-05\n",
      "Epoch 9/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1438 - auc: 0.8296\n",
      "Epoch 9: val_loss did not improve from 0.15263\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "22464/22464 [==============================] - 3979s 177ms/step - loss: 0.1438 - auc: 0.8296 - val_loss: 0.1545 - val_auc: 0.8279 - lr: 2.0000e-05\n",
      "Epoch 10/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1419 - auc: 0.8368\n",
      "Epoch 10: val_loss improved from 0.15263 to 0.15046, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4092s 182ms/step - loss: 0.1419 - auc: 0.8368 - val_loss: 0.1505 - val_auc: 0.8309 - lr: 4.0000e-06\n",
      "Epoch 11/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1413 - auc: 0.8401\n",
      "Epoch 11: val_loss improved from 0.15046 to 0.14898, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 4115s 183ms/step - loss: 0.1413 - auc: 0.8401 - val_loss: 0.1490 - val_auc: 0.8320 - lr: 4.0000e-06\n",
      "Epoch 12/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1412 - auc: 0.8405\n",
      "Epoch 12: val_loss did not improve from 0.14898\n",
      "22464/22464 [==============================] - 4013s 179ms/step - loss: 0.1412 - auc: 0.8405 - val_loss: 0.1497 - val_auc: 0.8329 - lr: 4.0000e-06\n",
      "Epoch 13/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1407 - auc: 0.8424\n",
      "Epoch 13: val_loss improved from 0.14898 to 0.14735, saving model to models\\best_model.keras\n",
      "22464/22464 [==============================] - 3972s 177ms/step - loss: 0.1407 - auc: 0.8424 - val_loss: 0.1473 - val_auc: 0.8344 - lr: 4.0000e-06\n",
      "Epoch 14/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1407 - auc: 0.8421\n",
      "Epoch 14: val_loss did not improve from 0.14735\n",
      "22464/22464 [==============================] - 4015s 179ms/step - loss: 0.1407 - auc: 0.8421 - val_loss: 0.1482 - val_auc: 0.8340 - lr: 4.0000e-06\n",
      "Epoch 15/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1404 - auc: 0.8450\n",
      "Epoch 15: val_loss did not improve from 0.14735\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "22464/22464 [==============================] - 4105s 183ms/step - loss: 0.1404 - auc: 0.8450 - val_loss: 0.1487 - val_auc: 0.8342 - lr: 4.0000e-06\n",
      "Epoch 16/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1400 - auc: 0.8446\n",
      "Epoch 16: val_loss did not improve from 0.14735\n",
      "22464/22464 [==============================] - 4317s 192ms/step - loss: 0.1400 - auc: 0.8446 - val_loss: 0.1485 - val_auc: 0.8340 - lr: 8.0000e-07\n",
      "Epoch 17/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1399 - auc: 0.8456\n",
      "Epoch 17: val_loss did not improve from 0.14735\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "22464/22464 [==============================] - 4550s 203ms/step - loss: 0.1399 - auc: 0.8456 - val_loss: 0.1489 - val_auc: 0.8329 - lr: 8.0000e-07\n",
      "Epoch 18/20\n",
      "22464/22464 [==============================] - ETA: 0s - loss: 0.1399 - auc: 0.8464\n",
      "Epoch 18: val_loss did not improve from 0.14735\n",
      "22464/22464 [==============================] - 4573s 204ms/step - loss: 0.1399 - auc: 0.8464 - val_loss: 0.1484 - val_auc: 0.8341 - lr: 1.6000e-07\n"
     ]
    }
   ],
   "source": [
    "# Calculating steps based on our batch size\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
    "\n",
    "print(\"Starting training... Go grab a coffee, this will take time!\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=20, # 20 is a good starting point, EarlyStopping will handle it\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f001c-db88-4bd5-915f-e98ba7db7c54",
   "metadata": {},
   "source": [
    "Technical Note on Training Termination (Early Stopping Analysis):\n",
    "\n",
    "The training process was intentionally terminated at Epoch 18 despite a maximum limit of 20 epochs. This behavior is attributed to the EarlyStopping callback with a monitored metric of val_loss and a patience setting of 5.Optimal Convergence: The model reached its peak performance (minimum validation loss) at Epoch 13.The 5-Epoch Rule: From Epoch 14 to 18, the model failed to achieve a lower val_loss than the record set in Epoch 13.Prevention of Overfitting: Upon reaching the threshold of 5 consecutive non-improving epochs, the system triggered a shutdown to prevent overfitting and \"weight drifting.\"Result: The final saved weights (best_model.keras) correspond to the state at Epoch 13, ensuring the most generalizable version of the network is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d137d2f5-4cd7-48df-ac8c-89dc62ef59c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History saved successfully as train_history.json!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert float32 values to standard python floats\n",
    "# We iterate through each key (like 'auc', 'loss') and convert its list of values\n",
    "history_dict = {\n",
    "    key: [float(i) for i in value] \n",
    "    for key, value in history.history.items()\n",
    "}\n",
    "\n",
    "# Now save it to your models folder - this will work perfectly!\n",
    "with open('models/train_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "\n",
    "print(\"History saved successfully as train_history.json!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Med-GPU)",
   "language": "python",
   "name": "med-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
